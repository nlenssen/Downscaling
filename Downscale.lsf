#!/bin/csh
#
# $Id: Downscale.lsf,v 1.8 2008/05/02 15:45:37 thoar Exp $
#
#BXXX -b 17:00
#BSUB -J "downscale[1-16]"
#BXXX -J "downscale[1-13]"
#BXXX -J "downscale[1]"
#BSUB -o downscale.%J.%I.log
#BSUB -q regular
#BXXX -q geyser
#BSUB -N -u ${USER}@ucar.edu
#BSUB -n 1
#BSUB -W 7:00
#BSUB -P P86850053

#
#----------------------------------------------------------------------
# End of script preamble.
#----------------------------------------------------------------------

if ($?LSB_HOSTS) then

   setenv ORIGINALDIR $LS_SUBCWD 
   setenv JOBNAME     $LSB_OUTPUTFILE:ar
   setenv JOBID       $LSB_JOBID
   setenv MYQUEUE     $LSB_QUEUE
   setenv MYHOST      $LSB_SUB_HOST
   setenv TASKID      $LSB_JOBINDEX
   setenv NTASKS      $LSB_JOBINDEX_END
   setenv TEMPDIR     /glade/scratch/lenssen/
   setenv DATADIR     /glade/p/work/lenssen/output/

else

   # You can debug the script by running this interactively.

   setenv ORIGINALDIR `pwd`
   setenv JOBNAME     downscale
   setenv JOBID       $$
   setenv MYQUEUE     Interactive
   setenv MYHOST      $HOST
   setenv TASKID      2
   setenv NTASKS      3
   setenv TEMPDIR     /glade/scratch/lenssen/
   setenv DATADIR     /glade/p/work/lenssen/output/

endif

#----------------------------------------------------------------------
# Set up the arrays that will be exploited by the Job Array ID 
#----------------------------------------------------------------------

set startyears = ( 1850 1860 1870 1880 1890 1900 1910 1920 \
		   1930 1940 1950 1960 1970 1980 1990 2000 )

set   endyears = ( 1859 1869 1879 1889 1899 1909 1919 1929 \
		   1939 1949 1959 1969 1979 1989 1999 2005 )

set year1 = $startyears[$TASKID]
set yearN = $endyears[$TASKID]   

#----------------------------------------------------------------------
# List of all candidate files to be downscaled for this experiment
# lrfname ... "Low Res filename"
#----------------------------------------------------------------------

set prismvar = tave
set   gcmvar = tas

# setenv ddir /glade/u/home/lenssen/downscaling/gcm
# setenv ddir /glade/u/home/lenssen/downscaling/TAS
  setenv ddir /glade/p/work/lenssen/AR5

set lrfname = ${gcmvar}_Amon_CCSM4_historical_r1i1p1_185001-200512.nc


echo "working on $lrfname decade ${TASKID} at "`date`
echo "running on $HOST in queue ${MYQUEUE}"
# Pick an input file and create the corresponding output file 
# that reflects the fact we're only doing 10 years here.
# The precip downscaling requires using the PRISM variable
# in the output file name, while the temp downscaling
# requires using the GCM variable name. Arghhh....

set OFNAME = `echo $lrfname | sed -e "s#0280#$year1#"`
set OFNAME = `echo  $OFNAME | sed -e "s#0509#$yearN#"`
#set OFNAME = `echo  $OFNAME | sed -e "s#${gcmvar}_#${prismvar}_#"`
set YEARARR = "(1850:2005)"

set OUTPUTBASE = ${OFNAME:r}.downscaled

set EXP = $lrfname:r
set EXP = $EXP:r
set EXP = $EXP:r
set EXP = $EXP:r
set EXP = $EXP:e

#----------------------------------------------------------------------
# Create a clean temporary directory, work in it, and clean up when done.
#----------------------------------------------------------------------

setenv TMPDIR ${TEMPDIR}/${JOBNAME}_${EXP}_job${TASKID}

mkdir -p ${TMPDIR}
cd ${TMPDIR}

#----------------------------------------------------------------------
# Just an echo of job attributes
# I am also leaving a trail of breadcrumbs in case the job fails.
# I'll be able to track which node was being used when it failed.
#----------------------------------------------------------------------

echo
echo "${JOBNAME} ($JOBID) submitted   from $ORIGINALDIR"
echo "${JOBNAME} ($JOBID) submitted   from $MYHOST"
echo "${JOBNAME} ($JOBID) running in queue $MYQUEUE"
echo "${JOBNAME} ($JOBID) running       on $HOST"
echo "${JOBNAME} ($JOBID) job $TASKID of $NTASKS started at "`date`
echo

echo "${JOBNAME} ($JOBID) submitted   from $ORIGINALDIR"            >! breadcrumb
echo "${JOBNAME} ($JOBID) submitted   from $MYHOST"                 >> breadcrumb
echo "${JOBNAME} ($JOBID) running in queue $MYQUEUE"                >> breadcrumb
echo "${JOBNAME} ($JOBID) running       on $HOST"                   >> breadcrumb
echo "${JOBNAME} ($JOBID) job $TASKID of $NTASKS started at "`date` >> breadcrumb

#----------------------------------------------------------------------
# Create the R program to run
#
# rather remarkably ... it was taking more than 30 minutes to create a netcdf
# file of 10 years ... and it takes less than a minute to do it for 1 year.
# So ... I might just process every year independently and ncrcat them together.
# the problem is that the high-water memory mark for a 400MB netCDF file
# is over 4GB ... crud.
#
# The output is EXPLICITLY created in the final resting place, so it does
# not need to be moved.
#----------------------------------------------------------------------

echo "   year1    = $year1"          >! ${JOBNAME}.${JOBID}.${TASKID}.r
echo "   yearN    = $yearN"          >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo '   prismvar = "'$prismvar'"'   >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo '   gcmvar   = "'$gcmvar'"'     >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo '   fname    = "'$lrfname'"'    >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo '   fbase    = "'$OUTPUTBASE'"' >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo '   ddir     = "'$ddir'"'       >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo '   dircoeff = "'$DATADIR'"'    >> ${JOBNAME}.${JOBID}.${TASKID}.r
echo "   yeararr  = $YEARARR"        >> ${JOBNAME}.${JOBID}.${TASKID}.r

cat << ENDofTask01 >> ${JOBNAME}.${JOBID}.${TASKID}.r

   library(fields)
   library(ncdf)

   source("/glade/u/home/lenssen/downscaling/GetPredDomain.r")
   source("/glade/u/home/lenssen/downscaling/ConvertUnits.r")
   source("/glade/u/home/lenssen/downscaling/CoeffsEveryToRData.r")
   source("/glade/u/home/lenssen/downscaling/SimpleRDataToNetCDF.r")

   ofname = sprintf("%s/%s.RData",".",fbase)

   ddata = CoeffsEveryToRData( year1      = year1,
                               yearN      = yearN,
                               prismvar   = prismvar,
                               coarsevar  = gcmvar,
                               coarsefile = fname,
                               ddir       = ddir,
                               dircoeff   = dircoeff,
                               yeararr    = yeararr)
   save(ddata,file=ofname)

#  load( ofname )
   ofname = paste(fbase,".nc",sep="")
   bob = SimpleRDataToNetCDF(vrbl = ddata,
                             ddir = ".",
                             file = ofname)

ENDofTask01
#----------------------------------------------------------------------
# Run the program:
#----------------------------------------------------------------------
R CMD BATCH --no-save ${JOBNAME}.${JOBID}.${TASKID}.r \
                      ${JOBNAME}.${JOBID}.${TASKID}.rout
   
#----------------------------------------------------------------------
# Copy the output back to where we started, remove temporary directory.
#----------------------------------------------------------------------

mv ${JOBNAME}.${JOBID}.${TASKID}.r    ${ORIGINALDIR}
mv ${JOBNAME}.${JOBID}.${TASKID}.rout ${ORIGINALDIR}
mv ${OUTPUTBASE}.RData                ${DATADIR}
mv ${OUTPUTBASE}.nc                   ${DATADIR}

ls -alh 

cd ${ORIGINALDIR}
\rm -rf ${TMPDIR}

echo ""
echo "${JOBNAME} ($JOBID) job $TASKID of $NTASKS finished at "`date`
echo ""

#----------------------------------------------------------------------
# LSF options for BSUB
#
### -J      job name 
### -o      output listing filename 
### -q      queue
### -N -u   mail this user when job finishes
### -n      number of processors  (really)
### -W      wall-clock hours:minutes required
### -b      [[month:]day:]hour:minute "Dispatches the job for 
###         execution on or after the specified date and time. The date 
###         and time are in the form of [[month:]day:]hour:minute where 
###         the number ranges are as follows:
###         month 1-12, day 1-31, hour 0-23, minute 0-59.
#
#----------------------------------------------------------------------

exit
